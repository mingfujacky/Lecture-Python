{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- Web scraping is the automated process of extracting data from the internet. \n",
    "- The Python libraries **Requests** and **Beautiful Soup** are powerful tools for the job.\n",
    "- 測試網址 URL = \"https://realpython.github.io/fake-jobs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Inspect data source\n",
    "- developer tools: Safari -> 開發 -> 顯示網頁檢閱器\n",
    "- developer tools: Chrome -> 檢視 -> 開發人員選項 -> 開發人員工具"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"left\">\n",
    "  <img src=\"https://realpython.com/cdn-cgi/image/width=1440,format=auto/https://files.realpython.com/media/bs4-devtools.f0a236ca5fa3.png\" \n",
    "  alt=\"網頁檢閱器\"\n",
    "  width=\"640\" \n",
    "  height=\"360\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Scrape HTML content from a page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 打開 URL\n",
    "- 印出 HTML 的內容  \n",
    "- Copy-paste儲存格輸出到 HTML Formatter 網站(https://htmlformatter.com)去協助整理格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = \"https://realpython.github.io/fake-jobs/\"\n",
    "response = requests.get(URL)\n",
    "print(\"Status code: \", response.status_code)\n",
    "print(\"Elapsed Time:\", response.elapsed)\n",
    "print(response.text)\n",
    "# if needed to paste response.text content to HTML formater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用HTML formatter 網站或是Safari 網頁檢閱器(檢閱元件)分析HTML發現\n",
    "- class=\"title is-5\": job title\n",
    "- class=\"subtitle is-6 company\": company\n",
    "- class=\"location\": location\n",
    "- class=\"is-small has-text-grey\": post date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Parse HTML code with Beautiful Soup\n",
    "- Find elements by ID\n",
    "- Find elements by class name\n",
    "- Extract text from HTML elements\n",
    "- Find elements by class name and text content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "sp = BeautifulSoup(response.content, \"html.parser\") # Parser parse 'response.content' better than 'response.text'\n",
    "print(sp.title); print(sp.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Find elements by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find elements by ID\n",
    "# In HTML, the id attribute is used to uniquely identify an element within the document.\n",
    "results = sp.find(id=\"ResultsContainer\")\n",
    "print(results.prettify()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Find elements by class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find elements by tag name and class name\n",
    "job_cards = results.find_all(\"div\", class_=\"card-content\")\n",
    "print(len(job_cards), '\\n')\n",
    "print(job_cards[0].prettify()[:600], '\\n') \n",
    "for job_card in job_cards:\n",
    "    print(job_card.prettify()[:600], end=\"\\n\" * 2)\n",
    "    print(\"=\" * 60, end=\"\\n\" * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all job listings\n",
    "print('JOB LISTINGS')\n",
    "for i, job_card in enumerate(job_cards):\n",
    "    title_element = job_card.find(\"h2\", class_=\"title is-5\")\n",
    "    company_element = job_card.find(\"h3\", class_=\"subtitle is-6 company\")\n",
    "    location_element = job_card.find(\"p\", class_=\"location\")\n",
    "    print(f\"Job {i}:\")\n",
    "    print(title_element.prettify())\n",
    "    print(company_element.prettify())\n",
    "    print(location_element.prettify())\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Extract Text From HTML Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Text From HTML Elements\n",
    "for i, job_card in enumerate(job_cards):\n",
    "    title_element = job_card.find(\"h2\", class_=\"title is-5\")\n",
    "    company_element = job_card.find(\"h3\", class_=\"subtitle is-6 company\")\n",
    "    location_element = job_card.find(\"p\", class_=\"location\")\n",
    "    print(f\"Job {i}:\")\n",
    "    print(title_element.text.strip())\n",
    "    print(company_element.text.strip())\n",
    "    print(location_element.text.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Print all Python-related job title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all Python related jobs\n",
    "# Find Elements by Class Name and Text Content\n",
    "# Passing a string (Python) to the string argument. This needs exact same match\n",
    "python_jobs = results.find_all(\"h2\", string=\"Python\")\n",
    "print(len(python_jobs))\n",
    "senior_python_developer_jobs = results.find_all(\"h2\", string=\"Senior Python Developer\")\n",
    "print(len(senior_python_developer_jobs))\n",
    "\n",
    "# Passing an lambda function to the string argument. It will return all the elements that return True.  \n",
    "python_jobs = results.find_all(\"h2\", string=lambda text: \"python\" in text.lower())\n",
    "print(len(python_jobs))\n",
    "for job_card in python_jobs:\n",
    "    print(job_card.text.strip()[:22], '\\t\\t', job_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Print all Python-related job title, company and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_jobs = results.find_all(\"h2\", string=lambda text: \"python\" in text.lower())\n",
    "python_job_cards = [h2_element.parent.parent.parent for h2_element in python_jobs]\n",
    "for job_card in python_job_cards:\n",
    "    title_element = job_card.find(\"h2\", class_=\"title is-5\")\n",
    "    company_element = job_card.find(\"h3\", class_=\"subtitle is-6 company\")\n",
    "    location_element = job_card.find(\"p\", class_=\"location\")\n",
    "    print(title_element.text.strip())\n",
    "    print(company_element.text.strip())\n",
    "    print(location_element.text.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Extract attributes from HTML elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_card in python_job_cards:\n",
    "    link_url = job_card.find_all(\"a\")[1][\"href\"]\n",
    "    print(f\"Apply here: {link_url}\\n\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Assemble code in a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://realpython.github.io/fake-jobs/\"\n",
    "response = requests.get(URL)\n",
    "\n",
    "sp = BeautifulSoup(response.content, \"html.parser\")\n",
    "results = sp.find(id=\"ResultsContainer\")\n",
    "\n",
    "python_jobs = results.find_all(\n",
    "    \"h2\", string=lambda text: \"python\" in text.lower()\n",
    ")\n",
    "\n",
    "python_job_cards = [\n",
    "    h2_element.parent.parent.parent for h2_element in python_jobs\n",
    "]\n",
    "\n",
    "for job_card in python_job_cards:\n",
    "    title_element = job_card.find(\"h2\", class_=\"title\")\n",
    "    company_element = job_card.find(\"h3\", class_=\"company\")\n",
    "    location_element = job_card.find(\"p\", class_=\"location\")\n",
    "    link_url = job_card.find_all(\"a\")[1][\"href\"]\n",
    "    print(title_element.text.strip())\n",
    "    print(company_element.text.strip())\n",
    "    print(location_element.text.strip())\n",
    "    print(f\"Apply here: {link_url}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python.org job board: https://www.python.org/jobs/\n",
    "- find job title and company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a URL and check the status code 200\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://www.python.org/jobs/\"\n",
    "response = requests.get(URL)\n",
    "print(\"Status code: \", response.status_code)\n",
    "print(\"Elapsed Time:\", response.elapsed)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content and check parsing successful\n",
    "sp = BeautifulSoup(response.content, \"html.parser\")\n",
    "print(sp.title); print(sp.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoping the job container\n",
    "results = sp.find(\"ol\", class_=\"list-recent-jobs list-row-container menu\")\n",
    "if results:\n",
    "    print(len(results.find_all(\"li\")))  # Count the number of <li> elements inside the <ol>\n",
    "else:\n",
    "    print(\"No job cards found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB LISTINGS\n",
      "Job 0:\n",
      "Python Software Engineer\n",
      "HypothesisBase\n",
      "--------------------------------------------------------------------------------\n",
      "Job 1:\n",
      "Python Lead Developer\n",
      "SenecaGlobal\n",
      "--------------------------------------------------------------------------------\n",
      "Job 2:\n",
      "Full Stack Python Developer\n",
      "TeraLumen Solutions Pvt Ltd\n",
      "--------------------------------------------------------------------------------\n",
      "Job 3:\n",
      "Senior Back-End Python Engineer\n",
      "ActivePrime, Inc.\n",
      "--------------------------------------------------------------------------------\n",
      "Job 4:\n",
      "Senior Python Engineer\n",
      "Kazang a company part of the Lesaka Technologies Group\n",
      "--------------------------------------------------------------------------------\n",
      "Job 5:\n",
      "Software Engineer - OpenStack Swift Object Storage\n",
      "Red Hat, Inc.\n",
      "--------------------------------------------------------------------------------\n",
      "Job 6:\n",
      "Porting code from Rust to Python on GPU\n",
      "NJB Brands LLC\n",
      "--------------------------------------------------------------------------------\n",
      "Job 7:\n",
      "Senior DevOps Engineer/Computer Scientist\n",
      "European Gravitational Observatory (EGO)\n",
      "--------------------------------------------------------------------------------\n",
      "Job 8:\n",
      "Data Administrator\n",
      "Pepper Wireless\n",
      "--------------------------------------------------------------------------------\n",
      "Job 9:\n",
      "Principal Software Engineer (d/f/m)\n",
      "Entrix GmbH\n",
      "--------------------------------------------------------------------------------\n",
      "Job 10:\n",
      "Python Software Developer\n",
      "Axion Recruitment\n",
      "--------------------------------------------------------------------------------\n",
      "Job 11:\n",
      "Senior Full Stack Developer\n",
      "Baserow\n",
      "--------------------------------------------------------------------------------\n",
      "Job 12:\n",
      "Expert python engineer for data science startup\n",
      "Windsor.ai\n",
      "--------------------------------------------------------------------------------\n",
      "Job 13:\n",
      "Lead Python Backend Engineer\n",
      "Reef Technologies\n",
      "--------------------------------------------------------------------------------\n",
      "Job 14:\n",
      "Senior Python Backend Engineer\n",
      "Reef Technologies\n",
      "--------------------------------------------------------------------------------\n",
      "Job 15:\n",
      "Principal Data Scientist\n",
      "eSImplicity\n",
      "--------------------------------------------------------------------------------\n",
      "Job 16:\n",
      "Senior Data Scientist\n",
      "eSImplicity\n",
      "--------------------------------------------------------------------------------\n",
      "Job 17:\n",
      "Principal Back End Engineer\n",
      "EDITED\n",
      "--------------------------------------------------------------------------------\n",
      "Job 18:\n",
      "Python Engineer – API and SaaS Application Development\n",
      "Aidentified, LLC\n",
      "--------------------------------------------------------------------------------\n",
      "Job 19:\n",
      "Python Developer\n",
      "Twipe\n",
      "--------------------------------------------------------------------------------\n",
      "Job 20:\n",
      "Expert python engineer for data science startup\n",
      "Windsor.ai\n",
      "--------------------------------------------------------------------------------\n",
      "Job 21:\n",
      "Econometrics Data Engineer\n",
      "Powerlytics\n",
      "--------------------------------------------------------------------------------\n",
      "Job 22:\n",
      "Data Engineer\n",
      "Powerlytics\n",
      "--------------------------------------------------------------------------------\n",
      "Job 23:\n",
      "Engineering Manager\n",
      "Multi Media LLC\n",
      "--------------------------------------------------------------------------------\n",
      "Job 24:\n",
      "Senior Software Engineer\n",
      "Multi Media LLC\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print all job listings\n",
    "print('JOB LISTINGS')\n",
    "for i, job_card in enumerate(job_cards):\n",
    "    title_element = job_card.find(\"a\").text\n",
    "    company_element = job_card.find(\"span\", class_=\"listing-company-name\").text.replace(\"\\t\", \"\").strip().split(\"\\n\")[-1]\n",
    "    print(f\"Job {i}:\")\n",
    "    print(title_element.strip())\n",
    "    print(company_element.strip())\n",
    "    print('-' * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
